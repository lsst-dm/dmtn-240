\documentclass[11pt,twoside]{article}
%https://pretalx.com/adass2022/me/submissions/8CYMWV/

% Do NOT use ANY packages other than asp2014.
\usepackage{asp2014}

\aspSuppressVolSlug
\resetcounters

% See ManuscriptInstructions.pdf for more details
\bibliographystyle{asp2014}


% The ``markboth'' line sets up the running heads for the paper.
% 1 author: "Surname"
% 2 authors: "Surname1 and Surname2"
% 3 authors: "Surname1, Surname2, and Surname3"
% >3 authors: "Surname1 et al."
% Replace ``Short Title'' with the actual paper title, shortened if necessary.
% Use mixed case type for the shortened title
% Ensure shortened title does not cause an overfull hbox LaTeX error
% See ASPmanual2010.pdf 2.1.4  and ManuscriptInstructions.pdf for more details
\markboth{O'Mullane et al.}{Rubin Software Architecture and Design}

\begin{document}
%\ssindex{protocols!TAP}
%\ssindex{protocols!GKE}
%\ssindex{protocols!GCP}
%\ssindex{observatories!ground-based!Rubin}

\title{Software Architecture and System Design of Rubin Observatory}

% Note the position of the comma between the author name and the
% affiliation number.
% Authors surnames should come after first names or initials, eg John Smith, or J. Smith.
% Author names should be separated by commas.
% The final author should be preceded by "and".
% Affiliations should not be repeated across multiple \affil commands. If several
% authors share an affiliation this should be in a single \affil which can then
% be referenced for several author names. If only one affiliation, no footnotes are needed.
% See ManuscriptInstructions.pdf and ASP's manual2010.pdf 3.1.4 for more details

%% Regenerate using:
%%    python $LSST_TEXMF_DIR/bin/db2authors.py > authors.tex


\input{authors}


\begin{abstract}
This paper covers some astronomy design patterns and perhaps some anti-patterns in astronomy. We will use our experience on several long projects such as Rubin Observatory, Gaia, SDSS,  UKIRT and JCMT to highlight some of the the things which worked and a few things that did not work so well.
\end{abstract}

%Slides on https://docs.google.com/presentation/d/1xCDWSaBNUEP-YM82K2-doM-yb0AKUGQExratsi3iFdA/edit#slide=id.g144caede871_0_0
\section{Introduction}

The Legacy Survey of Space and Time \citep{2019ApJ...873..111I} is "deep fast wide" optical/near-IR survey of half the sky in ugrizy bands to r 27.5 (36 nJy) based on 825 visits over a 10-year period.
Carried out by Rubin Observatory on Cerro Pach\'{o}n (alt. 2647m) Chile, the survey will produce around 100\,PB of data consisting of about a billion 16\,Mpix images, enabling measurements for 40 billion objects!

The observatory is due to go into operations in 2024.
The telescope mount assembly is due to be handed over by the end of 2022.
In 2023 the mirrors should be mounted with the commissioning camera (a single raft or about 5\% of the full camera size) to perform verification of the system.

We already routinely operate the Auxiliary telescope as both an imager and spectrograph, the later being its purpose in operations as a calibration aid. For a regularly updated key milestones see \citep{DMTN-232}.

This paper briefly describes our software design and architecture and provides some details on decisions made.

\section{System Vision}
The mission statement for Rubin Data Management is :
\begin{quote}
Stand up operable, maintainable, quality services to deliver high-quality LSST data products for science and education, all on time and within reasonable cost.

\end{quote}

Rubin Observatory will snap an image approximately every 40s (slew and settle + 30s exposure time).
This leads to around 20TB of images streaming off the mountain in Chile each night.
The 100Gbs network from Chile to the USA was an early investment of the project which is now finally in place (though the redundant link still has some pending work).
Using this network Rubin Data Management must get the images to SLAC within seven seconds.
Once at SLAC prompt processing commences, after year one this will include regular alerts within 60s of shutter close (see Section \ref{sec:prompt}).
After 80 hours images will be available to the data rights holders (see \citep{RDO-011} for more on data releases).
On a more or less annual cadence DM will reprocess all the images taken since the start of operations and release new catalogs and other products as defined in \citet{LSE-163} (see Section \ref{sec:DRP}).

Rubin’s LSST is not the first wide-field imaging survey but the combination of depth, area, and throughput make it uniquely challenging.
Everything is blended and many measurements are systematics limited, but
we are already testing  on precursors like Hyper Subprime Cam (HSC).


\subsection{Democratizing research in astronomy}

We feel open source software is key to open science: not just for traditional reproducibility arguments, but because it is a step toward inclusivity.
Being open is not enough though, we must also find ways to support researchers who are:

\begin{itemize}
\item Resource-poor (do not have the compute resources associated with major research universities)
\item Time-poor (have a high teaching load, few/no grad students or postdocs)
\item Work in liberal arts colleges, historically black colleges, or other places that lack a large peer network for technical and research support

\end{itemize}

Lowering the barrier to entry goes beyond data rights and even beyond software; it requires minimizing the investment (time, money, experience) necessary to meaningfully engage with the scientific questions that can be resolved with the data.


As we move to operations we wish to provide a quality experience to all of our users.
The Science Platform provides a level playing field for interacting with Rubin data.
You do need an internet connection and a browser but the load is all on our server side hence your institute does not need to have a super computer to allow sophisticated experimentation.



\section{Architecture}
 Figure \ref{fig:arch} shows a simplified view of the system architecture, the full details are publicly available in \citep{LDM-148}. All the DM code is available on github at \url{https://github.com/lsst}.


\begin{centering}
\articlefigure{I08_f1.eps}{fig:arch}{ Simplified  Vera C. Rubin Architecture diagram from Magic Draw \label{fig:arch}}
\end{centering}

DM's work commences once the image is read out of the Camera. DM already gathers some information which the Camera software puts in the image header to make a minimally meaningful image.
As the image is written the Camera software also provides a quick look  of the image.
Once available the image is written to the Observatory Operations Data Service (OODS) and simultaneously transferred to the US Data Facility (USDF) at SLAC via the Prompt Transfer System. Though we use Rucio for transfers between facilities we could not make it fast enough for the Prompt Transfer which is custom code. We will say a little more about prompt processing in Section \ref{sec:prompt}

On the summit there is a restricted access Science Platform which allows staff to interact with the  images in the OODS directly. A cluster of about 400 cores is available for quick adhoc processing in situ. However we expect most processing to be done at the USDF.

\subsection{Prompt Processing} \label{sec:prompt}
\subsection{Data Release Processing}\label{sec:DRP}

\subsection{Data Access} \label{sec:dataaccess}

Data rights holders will have access to the catalogs and images via the Science platform.
The catalogs are large for relational databases.
While the object catalog at $4 \times 10^{10}$ rows may be manageable
the source catalog (of all observations) will run to $ 10^{12}$ rows which is beyond conventional relational technology.
Qserv \citep{C15_adassxxxii} was built specifically to answer astro queries quickly for large numbers of users.
We have also tried non relational DBs including Google's BigQuery, Qserv provides the best cost service value for catalog queries (see e.g. \citet{Document-31100}.
However we see the advantage of non relational technology for
some  \emph{unpredictable} and \emph{complex} access which Qserv may or may not handle user-defined functions, pattern matching, unusual iteration schemes.
Many cloud tools work best with cloud formats like parquet.
So we are considering having parquet files along side Qserv

A subset of data will be public via Education and Public outreach including for citizen science projects.
Alert stream will be fully public.

We will hold most data at SLAC but run the science platform on Google \citep{2021arXiv211115030O}.
We choose this Hybrid model since Compute is cheap on the cloud but storage is expensive.
The ingress is free and the user egress is manageable.

\subsection{Cloud Native}\label{sec:cloudnative}
Like many projects \citep{2017ASPC..512...33O} Rubin leaned heavily on containers early on.
We also quickly understood the need for sophisticated container orchestration and settled on Kubernetes.
This decision drove service architectures that are well isolated from the underlying infrastructure.
This approach has already paid off massive dividends:

\begin{itemize}
\item When funding lines suddenly shifted we were able to painlessly transition from an on-premises facility to an Interim Data Facility on Google Cloud
\item The Rubin Science Platform (RSP) became a generic data services platform that is currently deployed on eight distinct (and distinctly managed) infrastructures (on-prem and cloud)
\item Cloud can now be freely leveraged for services like the RSP who benefit from its advantages such as elasticity, scalability, isolation
\end{itemize}

Our architectural approach is geared towards lowering the cost for developing and deploying a new data service.
Services utilize a common infrastructure providing services such as authentication and authorization, secrets management, Transport Layer Security (TLS) certificates, and templates to speed up creation of new services in the FastAPI framework.
The GitOps infrastructure for k8s deployment using ArgoCD takes care of easy per-infrastructure configuration and deployment.
This is internally named Phalanx.\footnote{\url{http://phalanx.lsst.io/}}

On the summit in Cerro Pachón where we have on prem machines the Chile DevOps team use Foreman and puppet to bring up s full K8S infrastructure.
Both DM and Telescope and Site software then deploy services on top of this infrastructure.
Deploying control components on K8S allows for better resilience.
There are of course some devices this can not be done for some of these are still deployed with puppet but not all.


\section{Lessons learned}
We would like to share some observations from this project in a number of areas.
\subsection{Standards}
Standards are excellent they minimize the learning curve for new hires which is a major problem on all software projects.
The European Cooperation for Space Standardization (ECSS) used for Gaia has been mentioned before \citep{2007arXiv0712.0249O} at ADASS.
 Rubin used Model Based System Engineering (MBSE) \citep{2018SPIE10705E..0US}.

For astronomy we have also have the International Virtual Observatory (IVOA) standards. Gaia archive is full IVOA based \citep{2019ASPC..523..445S,2015scop.confE...8G}. The Sloane Digital Sky Survey (SDSS) implemented and helped define many of the original protocols \citep{2004AAS...20511301T}.
Rubin is IVOA first implementing TAP and  HiPS,
our Image services implements SODA.
Rubin uses DataLink to abstract image access from ObsTAP results and
some microservices  bounce TAP results into additional queries via DataLink descriptors.

One upside of picking IVOA standards is that now there are many implementations available.
Rubin uses the CADC's TAP implementation  with our own Qserv plugin.
Users of the TAP service have no idea they are using Qserv.
That allows us to use Firefly for visualization fairly easily as it is
fully VO based.

\subsection{Architecture}
There is a lot of analysis and design to come up with ans architecture - standards help with that lots of people use UML.
Undoubtedly tools to support your chosen standards extremely useful in the beginning, later they may become cumbersome. Both Rubin and Gaia started out with Rational Architect and switched at some point to Magic Draw.
Rubin still maintains a full set of requirements and design elements in Magic Draw while Gaia switched to code as prime and reverse engineers some diagrams for documentation purposes.

For verification  Rubin uses  Jira Test Manager (Zephyr) while for Gaia we used an in house tracking system based on open software.
Some systematic approach is needed form the outset or this will get away from you quickly.
Making it as automated as possible is also essential from the outset.

One catch on both projects was to have clearly written and testable requirements, this is extremely difficult and neither project was stellar.
This may be due to an underestimation of system engineering needs.
We note that system engineering tends to not scale linearly with project size.
We can easily fall in the trap of assuming that all agree on vague statements or requirements when usually a little delving will show quite the opposite.
It is worth putting effort in early to write down in detail i and as precisely as possible how we think things will work.


However you do your initial breakdown is possibly immaterial they all work and you will inevitably end up with $7 {{+}\over{-}} 2$ subsystems.
You will end up with a a range of client server, model view controller, tiers, pipes and filters.
If its big enough you end up with a bit of almost everything.
Frequently large project start all subsystems together but
a slower ramp up is better in some cases - think when you need things, admittedly it is  hard to keep politics out of that sort of decision.
For Gaia Coordination Unit 1 ( CU1 Architecture) and CU3 (Astrometry) were concentrated on initially with others trailing by some months - people from other CUs were in CU1.
CU9 (Gaia Archive ) was purposefully delayed to many years after the other parts of the project were almost built and launch was close.
On Rubin all DM WBS elements started together
some  teams were ready but others did not need their components  yet.
It did lead to good involvement in developer guide and project management approach.



\subsection{Documentation}
It is highly recommended to have  a good document publishing and indexing system.
Provide templates for standard documents early on to make it easy for people to follow the standards. Texmf is a good way to do this for \LaTeX.
Most Gaia docs used Latex templates provided and were in SVN,  Livelink held PDFs and the search system worked to some extent.
Metadata in Livelink was curated, not every one was allowed to upload documents.

Rubin developed a documentation infrastructure that further lowers the barrier to documentation by providing templated creation via Slack and uses the same IDE/toolchain developers use for coding, supports Restructured Text/Latex and is published via Github through the well-known GitOps workflow.
Single page documents (technotes) and site-based documentation (e.g. \url{ pipelines.lsst.io}) share the same infrastructure (see \url{sqr-000.lsst.io}) and search indexing hub (\url{www.lsst.io})
This has also minimizes useful information being consigned in hard to find (e.g. Google docs) and easy to forget or edit in one’s preferred IDE (eg wikis)
Docushare holds change controlled docs (PDFs, Word) has a search function though everyone has difficulty finding document in it.
This may be because the metadata is often incomplete or incorrect.

Both Rubin and Gaia have  bibfile generation for all recorded docs : Gaia using   livelink is  complete. Rubin uses \url{lsst.io} incomplete.
The Livelink API allowed the construction of the bibfile easily while we have failed to crack the Docushare API for Rubin plus the metadata is less than perfect.

Glossaries are good and it is great to start them early and get everyone using them.
Both Rubin and Gaia also have tools to generate acronym lists from documents (text, or tex - not Word).\footnote{\url{http://gaia.esac.esa.int/gpdb/glossary.txt}, \url{https://www.lsst.org/scientists/glossary-acronyms}}


\subsection{Interfaces}
A core tenet : \emph{Separate the data model from the persistence mechanism}
Algorithms should not access data directly.
Butler is great for this in Rubin, it is  now used on SPHEREx also (\cite{2022SPIE12189E..11J} , \cite{2019ASPC..523..653J}, Lust 2022).
Felis holds the model in Yaml, Butler passes Python Objects to clients with algorithm code not knowing about data location or file formats.
Similarly Gaia had data trains and the Main DataBase (MDB) dictionary which insulated algorithms from data access since the outset \citep{1999BaltA...8...57O}.
For Gaia the MDB dictionary holds all data models  \citep{2015ASPC..495...47H, 2011ASPC..442..351O}


Then there are the interfaces between systems and to others these are controlled by Interface Requirements Documents (IRD) and Interface Control Documents (ICD).
These need system engineering and test plans from the outset.
Rubin ICDs are really only IRDs .. but we may survive.
For Gaia  we placed too much faith in data ICDs, it was insufficient and we had to do work later to make data transfers and processing work well.



\subsection{Products, repositories and technology stacks}
\subsection{Deployment}
\subsection{Databases}
\subsection{Open software project management}
That's an entire book perhaps

Some of the software best practices are given in \citet{2018SPIE10707E..09J}.

\bibliography{I08}
\end{document}
