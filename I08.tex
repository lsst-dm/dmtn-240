\documentclass[11pt,twoside]{article}
%https://pretalx.com/adass2022/me/submissions/8CYMWV/

% Do NOT use ANY packages other than asp2014.
\usepackage{asp2014}

\aspSuppressVolSlug
\resetcounters

% See ManuscriptInstructions.pdf for more details
\bibliographystyle{asp2014}


% The ``markboth'' line sets up the running heads for the paper.
% 1 author: "Surname"
% 2 authors: "Surname1 and Surname2"
% 3 authors: "Surname1, Surname2, and Surname3"
% >3 authors: "Surname1 et al."
% Replace ``Short Title'' with the actual paper title, shortened if necessary.
% Use mixed case type for the shortened title
% Ensure shortened title does not cause an overfull hbox LaTeX error
% See ASPmanual2010.pdf 2.1.4  and ManuscriptInstructions.pdf for more details
\markboth{O'Mullane et al.}{Rubin Software Architecture and Design}

\begin{document}
%\ssindex{protocols!TAP}
%\ssindex{protocols!GKE}
%\ssindex{protocols!GCP}
%\ssindex{observatories!ground-based!Rubin}

\title{Software Architecture and System Design of Rubin Observatory}

% Note the position of the comma between the author name and the
% affiliation number.
% Authors surnames should come after first names or initials, eg John Smith, or J. Smith.
% Author names should be separated by commas.
% The final author should be preceded by "and".
% Affiliations should not be repeated across multiple \affil commands. If several
% authors share an affiliation this should be in a single \affil which can then
% be referenced for several author names. If only one affiliation, no footnotes are needed.
% See ManuscriptInstructions.pdf and ASP's manual2010.pdf 3.1.4 for more details

%% Regenerate using:
%%    python $LSST_TEXMF_DIR/bin/db2authors.py > authors.tex


\input{authors}


\begin{abstract}
This paper covers some astronomy design patterns and perhaps some anti-patterns in astronomy. We will use our experience on several long projects such as Rubin Observatory, Gaia, SDSS,  UKIRT and JCMT to highlight some of the the things which worked and a few things that did not work so well.
\end{abstract}

%Slides on https://docs.google.com/presentation/d/1xCDWSaBNUEP-YM82K2-doM-yb0AKUGQExratsi3iFdA/edit#slide=id.g144caede871_0_0
\section{Introduction}

The Legacy Survey of Space and Time \citep{2019ApJ...873..111I} is "deep fast wide" optical/near-IR survey of half the sky in ugrizy bands to r 27.5 (36 nJy) based on 825 visits over a 10-year period.
Carried out by Rubin Observatory on Cerro Pach\'{o}n (alt. 2647m) Chile, the survey will produce around 100\,PB of data consisting of about a billion 16\,Mpix images, enabling measurements for 40 billion objects!

The observatory is due to go into operations in 2024.
The telescope mount assembly is due to be handed over by the end of 2022.
In 2023 the mirrors should be mounted with the commissioning camera (a single raft or about 5\% of the full camera size) to perform verification of the system.

We already routinely operate the Auxiliary telescope as both an imager and spectrograph, the later being its purpose in operations as a calibration aid. For a regularly updated key milestones see \citep{DMTN-232}.

This paper briefly describes our software design and architecture and provides some details on decisions made.

\section{System Vision}
The mission statement for Rubin Data Management is :
\begin{quote}
Stand up operable, maintainable, quality services to deliver high-quality LSST data products for science and education, all on time and within reasonable cost.

\end{quote}

Rubin Observatory will snap an image approximately every 40s (slew and settle + 30s exposure time).
This leads to around 20TB of images streaming off the mountain in Chile each night.
The 100Gbs network from Chile to the USA was an early investment of the project which is now finally in place (though the redundant link still has some pending work).
Using this network Rubin Data Management must get the images to SLAC within seven seconds.
Once at SLAC prompt processing commences see Section \ref{sec:prompt}.
After 80 hours images will be available to the data rights holders (see \citep{RDO-011} for more on data releases).
On a more or less annual cadence DM will reprocess all the images taken since the start of operations and release new catalogs and other products as defined in \citet{LSE-163} (see Section \ref{sec:DRP}).

Rubinâ€™s LSST is not the first wide-field imaging survey but the combination of depth, area, and throughput make it uniquely challenging.
Everything is blended and many measurements are systematics limited, but
we are already testing  on precursors like Hyper Subprime Cam (HSC).


\subsection{Democratizing research in astronomy}

We feel open source software is key to open science: not just for traditional reproducibility arguments, but because it is a step toward inclusivity as
open data alone is not enough.
We must also find ways to support researchers who are:

\begin{itemize}
\item Resource-poor (do not have the compute resources associated with major research universities)
\item Time-poor (have a high teaching load, few/no grad students or postdocs)
\item Work in liberal arts colleges, historically black colleges, or other places that lack a large peer network for technical and research support

\end{itemize}

Lowering the barrier to entry  requires minimizing the investment (time, money, experience) necessary to meaningfully engage with the scientific questions that can be resolved with the data.
As we move to operations we wish to provide a quality experience to all of our users.
The Science Platform provides a level playing field for interacting with Rubin data.
You do need an internet connection and a browser but the load is all on our server side hence your institute does not need to have a super computer to allow sophisticated experimentation.


\section{Architecture}
 Figure \ref{fig:arch} shows a simplified view of the system architecture, the full details are publicly available in \citep{LDM-148}. All the DM code is available on github at \url{https://github.com/lsst}.


\begin{centering}
\articlefigure{I08_f1.eps}{fig:arch}{ Simplified  Vera C. Rubin Architecture diagram from Magic Draw }
\end{centering}

DM's work commences once the image is read out of the Camera. DM already gathers some information which the Camera software puts in the image header to make a minimally meaningful image.
As the data is written the Camera software also provides a quick look  of the image.
Once available the image is written to the Observatory Operations Data Service (OODS) and simultaneously transferred to the US Data Facility (USDF) at SLAC via the Prompt Transfer System. Though we use Rucio for transfers between facilities we could not make it fast enough for the Prompt Transfer which is custom code.
We will say a little more about prompt processing in Section \ref{sec:prompt}

On the summit there is a restricted access Science Platform which allows staff to interact with the  images in the OODS directly.
A cluster of about 400 cores is available for quick adhoc processing in situ.
However we expect most processing to be done at the USDF.

\subsection{Prompt Processing} \label{sec:prompt}
The Prompt Processing framework runs at the USDF, though many of the components of the framework will be reused to drive rapid analysis and quick-look functionality at the Summit and test stand facilities.
The design of Prompt Processing is driven be the requirement that alerts be distributed within 60 seconds of completion of readout of the last exposure of the visit.
To enable as much I/O and computation as possible to be done in advance, we instantiate one process per CCD when the summit sends a next\_visit Kafka event.
These next\_visit events providing notice of the telescope pointing, exposure duration, filter selection, and other metadata at least 20 seconds in advance of the first exposure of a visit.
Upon receiving the next\_visit event, we use knative in a Kubernetes environment to prepare a new container where we connect to the Butler to pre-load reference catalogs, calibration products, templates, solar system ephemerides, and prior alert history.
Once the raw images corresponding to an earlier next\_visit event for a given detector finish downloading to a local Ceph object store, the images are ingested to the container-local Butler and the Alert Production pipeline payload begins processing.
Other payloads, such as the Commissioning pipelines or Calibration Products pipelines, can also be run using the framework.
The Alert Production pipeline produces packaged alerts that are streamed to the Community Brokers, writes all data products to the repository at the USDF with the Butler, and updates the Alert Pipeline Database (APDB) with new measurements.
Detailed information on the initial design and prototype in the Google Cloud environment can be found in \citet{DMTN-219}.
Figure \ref{fig:pp} provides flow chart for prompt processing.

\begin{centering}
\articlefigure[width={4cm},angle=-90]{I08_f3.eps}{fig:pp}{ Prompt Processing flow diagram.}
\end{centering}

\subsection{Data Release Processing}\label{sec:DRP}
About once per year we will reprocess all data from the start of the survey.
We will spend a few months in precursor runs and QA before starting the nine month processing ordeal.
The jobs for this are distributed between France, US and UK using PanDA \citep{DMTN-213}
Figure \ref{fig:drp} provides an event chart for this.

\begin{centering}
\articlefigure[width={6cm},angle=-90]{I08_f2.eps}{fig:drp}{ DRP event chart from Magic Draw }
\end{centering}


\subsection{Data Access} \label{sec:dataaccess}

Data rights holders will have access to the catalogs and images via the Science platform.
The catalogs are large for relational databases.
While the object catalog at $4 \times 10^{10}$ rows may be manageable
the source catalog (of all observations) will run to $ 10^{12}$ rows which is beyond conventional relational technology.
Qserv \citep{C15_adassxxxii} was built specifically to answer astro queries quickly for large numbers of users.
We have also tried non relational DBs including Google's BigQuery but Qserv provides the best value for catalog queries (see e.g. \citet{Document-31100}.
However we see the advantage of non relational technology for
some  \emph{unpredictable} and \emph{complex} access which Qserv may or may not handle user-defined functions, pattern matching, unusual iteration schemes.
In fact many cloud tools work best with cloud formats like parquet and
 we are considering having parquet files along side Qserv for use with Spark or DASK.

A subset of data will be public via Education and Public outreach including for citizen science projects.
The Alert stream will be fully public.

We will hold most data at SLAC but run the science platform on Google \citep{2021arXiv211115030O}.
We choose this Hybrid model since Compute is cheap on the cloud but storage is expensive.
The ingress is free and the user egress is manageable.

\subsection{Cloud Native}\label{sec:cloudnative}
Like many projects \citep{2017ASPC..512...33O} Rubin leaned heavily on containers early on.
We also quickly understood the need for sophisticated container orchestration and settled on Kubernetes.
This decision drove service architectures that are well isolated from the underlying infrastructure.
This approach has already paid off massive dividends:

\begin{itemize}
\item When funding lines suddenly shifted we were able to painlessly transition from an on-premises facility to an Interim Data Facility on Google Cloud
\item The Rubin Science Platform (RSP) became a generic data services platform that is currently deployed on eight distinct (and distinctly managed) infrastructures (on-prem and cloud)
\item Cloud can now be freely leveraged for services like the RSP who benefit from its advantages such as elasticity, scalability, isolation
\end{itemize}

Our architectural approach is geared towards lowering the cost for developing and deploying a new data service.
Services utilize a common infrastructure (Phalanx\footnote{\url{http://phalanx.lsst.io/}}) providing services such as authentication and authorization, secrets management, Transport Layer Security (TLS) certificates, and templates to speed up creation of new services in the FastAPI framework.
The GitOps infrastructure for K8S deployment using ArgoCD takes care of easy per-infrastructure configuration and deployment.

On the summit in Cerro PachÃ³n where we have on prem machines the Chile DevOps team use Foreman and puppet to bring up a full K8S infrastructure.
Both DM and Telescope and Site software then deploy services on top of this infrastructure.
Deploying control components on K8S allows for better resilience.
There are of course some devices for which this approach does not work.


\section{Lessons learned}
We would like to share some observations from this project in a number of areas.
\subsection{Standards}
Standards are excellent they minimize the learning curve for new hires which is a major problem on all software projects.
The European Cooperation for Space Standardization (ECSS) used for Gaia has been mentioned before \citep{2007arXiv0712.0249O} at ADASS.
 Rubin used Model Based System Engineering (MBSE) \citep{2018SPIE10705E..0US}.

For astronomy we have also have the International Virtual Observatory (IVOA) standards. Gaia archive is full IVOA based \citep{2019ASPC..523..445S,2015scop.confE...8G}. The Sloane Digital Sky Survey (SDSS) implemented and helped define many of the original protocols \citep{2004AAS...20511301T}.
Rubin is IVOA first implementing TAP and  HiPS,
our Image services implements SODA.
Rubin uses DataLink to abstract image access from ObsTAP results and
some microservices  bounce TAP results into additional queries via DataLink descriptors.

One upside of picking IVOA standards is that now there are many implementations available.
Rubin uses the CADC's TAP implementation  with our own Qserv plugin.
Users of the TAP service have no idea they are using Qserv.
That allows us to use Firefly for visualization fairly easily as it is
fully VO based.

\subsection{Architecture}
There is a lot of analysis and design to come up with ans architecture - standards help with that lots of people use UML.
Undoubtedly tools to support your chosen standards extremely useful in the beginning, later they may become cumbersome. Both Rubin and Gaia started out with Rational Architect and switched at some point to Magic Draw.
Rubin still maintains a full set of requirements and design elements in Magic Draw while Gaia switched to code as prime and reverse engineers some diagrams for documentation purposes.

For verification  Rubin uses  Jira Test Manager \citep{2018SPIE10705E..0US}  while  Gaia  used an in house tracking system based on open software \citep{2012SPIE.8449E..0GC}.
A systematic and largely automated approach to verification is needed form the outset .

One catch on both projects was to have clearly written and testable requirements, this is extremely difficult and neither project was stellar.
This may be due to an underestimation of system engineering needs.
We note that system engineering tends to not scale linearly with project size.
We can easily fall in the trap of assuming that all agree on vague statements or requirements when usually a little delving will show quite the opposite.
It is worth putting effort in early to write down in detail i and as precisely as possible how we think things will work.


However you do your initial breakdown is possibly immaterial they all work and you will inevitably end up with $7 {{+}\over{-}} 2$ subsystems.
You will end up with a a range of client server, model view controller, tiers, pipes and filters.
If its big enough you end up with a bit of almost everything.
Frequently large project start all subsystems together but
a slower ramp up is better in some cases - think when you need things, admittedly it is  hard to keep politics out of that sort of decision.
For Gaia Coordination Unit 1 ( CU1 Architecture) and CU3 (Astrometry) were concentrated on initially with others trailing by some months - people from other CUs were in CU1.
CU9 (Gaia Archive ) was purposefully delayed to many years after the other parts of the project were almost built and launch was close.
On Rubin all DM WBS elements started together
some  teams were ready but others did not need their components  yet.
It did lead to good involvement in developer guide and project management approach.



\subsection{Documentation}
It is highly recommended to have  a good document publishing and indexing system.
Provide templates for standard documents early on to make it easy for people to follow the standards. Texmf is a good way to do this for \LaTeX.
Most Gaia docs used Latex templates provided and were in SVN,  Livelink held PDFs and the search system worked to some extent.
Metadata in Livelink was curated, not every one was allowed to upload documents.

Rubin developed a documentation infrastructure that further lowers the barrier to documentation by providing templated creation via Slack and uses the same IDE/toolchain developers use for coding, supports Restructured Text/Latex and is published via Github through the well-known GitOps workflow.
Single page documents (technotes) and site-based documentation (e.g. \url{ pipelines.lsst.io}) share the same infrastructure \citep{SQR-000} and search indexing hub (\url{www.lsst.io})
This has also minimizes useful information being consigned in hard to find (e.g. Google docs) and easy to forget or edit in oneâ€™s preferred IDE (eg wikis)
Docushare holds change controlled docs (PDFs, Word) has a search function though everyone has difficulty finding document in it.
This may be because the metadata is often incomplete or incorrect.

Both Rubin and Gaia have  bibfile generation for all recorded docs : Gaia using   livelink is  complete. Rubin uses \url{lsst.io} incomplete.
The Livelink API allowed the construction of the bibfile easily while we have failed to crack the Docushare API for Rubin plus the metadata is less than perfect.

Glossaries are good and it is great to start them early and get everyone using them.
Both Rubin and Gaia also have tools to generate acronym lists from documents (text, or tex - not Word).\footnote{\url{http://gaia.esac.esa.int/gpdb/glossary.txt}, \url{https://www.lsst.org/scientists/glossary-acronyms}}


\subsection{Interfaces}
A core tenet : \emph{Separate the data model from the persistence mechanism}
Algorithms should not access data directly.
Butler is great for this in Rubin, it is  now used on SPHEREx also (\cite{2022SPIE12189E..11J} , \cite{2019ASPC..523..653J}, \cite{C24_adassxxxii}, \cite{P52_adassxxxii}).
Felis holds the model in Yaml, Butler passes Python Objects to clients with algorithm code not knowing about data location or file formats.
Similarly Gaia had data trains and the Main DataBase (MDB) dictionary which insulated algorithms from data access since the outset \citep{1999BaltA...8...57O}.
For Gaia the MDB dictionary holds all data models  \citep{2015ASPC..495...47H, 2011ASPC..442..351O}


Then there are the interfaces between systems and to others these are controlled by Interface Requirements Documents (IRD) and Interface Control Documents (ICD).
These need system engineering and test plans from the outset.
Rubin ICDs are really only IRDs .. but we may survive.
For Gaia  we placed too much faith in data ICDs, it was insufficient and we had to do work later to make data transfers and processing work well.



\subsection{Products, repositories and technology stacks}
Not all projects use product trees but they can be very useful.
The Rubin product tree identified all  software products  and who is responsible for them in construction.
This was good but infrequently updated (partially perhaps since it was in magic draw).It should help group packages into products and help dependencies.
However on Rubin we have 100s of repos on GitHub, the dependencies are not straight forward and few are usable standalone.
We pretty much need to build a complete set from source.
So we have a Mono Build suited to a mono svn type repo but we have a package based set of GitHub repos.
Clearly the latter is the correct pattern getting there is hard.
The middleware has finally been made independent and put on PyPI which allowed SPHEREx to adopt it also.
The use of Conda environments has allowed improvements but it started late and we have not been able to turn the tide.
We did stop having patched versions for $3^{rd}$ party packages.

Gaia has a huge SVN repo of everything based roughly on product tree.  Builds are done on parts of the SVN tree - dependencies strictly managed at jar level via Nexus.
Middleware was extracted and put on PyPI.

Printed out full product trees are impressive to see the amount of work to do and are useful at early reviews.

Some of the software best practices are given in \citet{2018SPIE10707E..09J}.

\subsection{Deployment}
As mentioned (Section \ref{sec:cloudnative}) we are cloud native on Rubin.
Abstracting infrastructure effectively (Kubernetes / container orchestration, middleware etc) facilitates wider adoption of software and services by others, reducing context switching penalty and supporting continuing expertise.

Rubin uses Foreman/Puppet for bare metal setup security injection etc. up to kubernetes. Some machines on the summit are not under kubernetes of course.
We started late and SLAC were already using Chef and continue to do so - having both of these is unfortunate on one project.

Rubin Docker Hub for containers Kubernetes  for orchestration even  on the summit, ArgoCD is used to deploy everything. The RSP is deployed at eight distinct locations using ArgoCD.


Gaia choose Java for portability  (and ease of coding).
There were no containers but Jars were always deployed from Nexus.
All configurations for various machines were in SVN  deployment scripts pulled correct versions to a specific machine.

\subsection{Databases}
Some people love them and some people hate them some of us love them and hate them but datasets are a part of any big system and choosing the correct one is hard.
We think databases are great for persistence, the ability to query in different ways and relational support for Atomicity, Consistency, Isolation, and Durability(ACID) .
But there are problems with centralization/replication, schema evolution, performance cliffs.
We have  difficulties with multi-user/multi-tenant systems.
REST APIs in front help a lot.

There is also a mistake often made of trying to use one single database -
more are better per-application databases, sometimes specialized (Redis, InfluxDB) add resilience and are more manageable nowadays.
On Rubin we have Influx for summit Engineering, Postgres for observing logs and ancillary info and AlertsDB.
We use Cassandra for Prompt Products and of course we have in house developed Qserv for catalog access \citep{C15_adassxxxii}.
Gaia had at least Intersystems Cache for processing \cite{2011ExA....31..215O}, Postgres for archive and the dictionary .

\subsection{Open software project management}
It seems appropriate to mention management here though that could be a topic for a full paper or book of its own.
In fact many insights may be found in \citet{OMULLANE2005}.
First we should be clear leadership is required for complex astronomy/software projects not just management.
Finding good leaders is very hard.
That requires domain expertise and management training
We try to help by spreading some management across several people, exposing them to the issues in the large project and ways to deal with them.
Most importantly provide support to potential managers/leaders.
One must  acknowledge this route is not for everyone  - experimenting is good but give people a route back in a short timescale if they decide its not all they wanted.

You can do agile but you probably want earned value \citep{2014SPIE.9150E..1EG,2016SPIE.9911E..0NK} to help understand what is being delivered and since NSF etc require it.
Have good managers who understand both technical and managerial needs, super hard.
Build a techno/scientific  culture in leadership, again not trivial.
We should be offering opportunities for getting career credit for supporting the mission and its community, not just first-to-publish.

There are lots of parties and institutes in big projects.
If you want to do open source put it in the contracts/agreements from the outset.
Licensing is important - Rubin picked GPL but would now prefer APL - hard to change
so do not make it TOO explicit in the contracts.
Create community activity and collaboration around a codebase if you can.




\bibliography{I08}
\noindent {\tiny This material or work is supported in part by the National Science Foundation through Cooperative Agreement AST-1258333 and Cooperative Support Agreement AST1836783 managed by the Association of Universities for Research in Astronomy (AURA), and the Department of Energy under Contract No. DE-AC02-76SF00515 with the SLAC National Accelerator Laboratory managed by Stanford University.
}
\end{document}
